# Data Analysis using LLMs

## v0 

-----------
`POST` `api/warehouse/ask/`

-----------
`POST` `api/warehouse/ask/{new_session_id}/save/`

-----------
`POST` `api/warehouse/ask/{session_id}/feedback/`

-----------
`GET` `api/warehouse/ask/sessions`

-----------
`GET` `api/warehouse/sync_tables`

-----------
## v1 (March 2025)

-----------
`POST` `api/warehouse/v1/ask/`

This endpoint takes a natural language `user_prompt` and generates a SQL query which (might) answer it. The SQL is generated by Vanna.

The org's warehouse needs to already have been used to train Vanna's RAG for this org.

Since this process can be time-consuming, the request handler offloads it to Celery. 

Celery task: `generate_sql_from_prompt_asked_on_warehouse` 

This task creates an `LlmSession` object of type `LONG_TEXT_SUMMARIZATION` in the database. The following data is saved into this session object:
- `Org`
- `OrgUser` who made the HTTP request
- a unique `session_id`
- the request id

The task then calls `warehousefunctions.generate_sql_from_warehouse_rag` to fetch the SQL.

`generate_sql_from_warehouse_rag` does the following:
1. Checks that the `org` has already been set up for this feature, by ensuring the existence of `Org.pgvector_creds` which is a lookup id in the secrets manager
2. Fetches the credentials from the secrets manager
3. Checks that the RAG is trained (the LLM service exposes `/api/vanna/train/check` to do this check)
4. Ask the LLM service to generate the SQL query. This is done by hitting `/api/vanna/ask` at the LLM service's end

For the LLM service to generate this SQL query, it needs both credentials to the warehouse as well as to the RAG database which Vanna uses

-----------
`POST` `api/warehouse/v1/ask/{session_id}/summarize/`

Fetches the `LlmSession` object having the provided `session_id`. If the job is still running we return, otherwise we launch the Celery task `summarize_warehouse_results`

This task takes the SQL prompt passed in and runs it against the org's warehouse. These rows are then uploaded to OpenAI's FileSearch (via the LLM service) and the user's prompt passed over as a query to the LLM. We send our `LONG_TEXT_SUMMARIZATION` prompt as the `assistant prompt` (like a `system prompt`) along with the user's.

... and then we poll the LLM service which polls OpenAI, and eventually it either works or it doesn't and the `LlmSession.session_status` is updated accordingly. In case it does work, we save the response in `LlmService.response` which is a list of `(prompt, response)` pairs

-----------
`POST` `api/warehouse/v1/ask/{session_id}/save/`

Using the `session_id`, fetches the `LlmSession` object of type `LONG_TEXT_SUMMARIZATION` and attaches a `session_name` to it.

There is also an "overwrite" functionality wherein an older session ID is passed in, and the `OrgUser` from that session is copied over to the current one

-----------
`POST` `api/warehouse/rag/train/`

This invokes `warehousefunctions.train_rag_on_warehouse`. This should probably run via a Celery task (?) [TODO]

We require that
1. The `pgvector_creds` for the `org` exist in the secrets manager
2. The training SQL exist in the `OrgWarehouseRagTraining` table

If the RAG training query doesn't exist, create it by running `warehousefunctions.scaffold_rag_training`. It needs the schemas, tables and columns to exclude from training. The helper `warehousefunctions.generate_training_sql` takes care of warehouse-specific SQL dialects.

The training SQL is then sent over to the LLM service. The LLM service will run the following using the Vanna SDK:

```
  df_information_schema = self.vanna.run_sql(training_sql)
  plan = self.vanna.get_training_plan_generic(df_information_schema)
  self.vanna.train(plan=plan)
```

-----------
`POST` `api/warehouse/table_data/run_sql/`

This takes a SQL SELECT query along with an optional limit and offset and runs the query with those parameters.


-----------
`POST` `api/warehouse/row_count/sql/`

This takes a SQL SELECT query and returns the total number of rows in the (non-paginated) resultset.

